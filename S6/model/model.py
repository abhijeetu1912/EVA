# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C0rTjgyQ2IILobw9Vq8uODyEgeggtpaM
"""

# load required libraries
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F



# construct cnn class
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
       
        # convolution block 1
        self.conv1a = nn.Conv2d(3, 64, kernel_size = 3, padding = 1)                 # convolution | 32*32*3 -> 32*32*128 | RF = 3
        self.conv1b = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)                # convolution | 32*32*128 -> 32*32*64 | RF = 5
        self.conv1c = nn.Conv2d(64, 32, kernel_size = 3, padding = 1, stride = 2)    # convolution | 32*32*64 -> 16*16*64 | RF = 7

        # convolution block 2 - depthwise separable convolution + dilation
        self.conv2a = nn.Conv2d(32, 64, kernel_size = 3, padding = 1, groups = 32)   # convolution | 16*16*32 -> 16*16*64 | RF = 11
        self.conv2b = nn.Conv2d(64, 32, kernel_size = 1)                             # convolution | 16*16*64 -> 16*16*32 | RF = 11       
        self.conv2c = nn.Conv2d(32, 64, kernel_size = 3, padding = 2, dilation = 2)  # convolution | 16*16*64 -> 16*16*32 | RF = 19      
        self.conv2d = nn.Conv2d(64, 32, kernel_size = 3, padding = 1, stride = 2)    # convolution | 16*16*64 -> 8*8*32 | RF = 23
        
        # convolution block 3 - depthwise separable convolution + dilation
        self.conv3a = nn.Conv2d(32, 64, kernel_size = 3, padding = 1, groups = 32)   # convolution | 8*8*32 -> 8*8*64 | RF = 31
        self.conv3b = nn.Conv2d(64, 32, kernel_size = 1)                             # convolution | 8*8*64 -> 8*8*32 | RF = 31       
        self.conv3c = nn.Conv2d(32, 64, kernel_size = 3, padding = 2, dilation = 2)  # convolution | 8*8*64 -> 8*8*32 | RF = 47        
        self.conv3d = nn.Conv2d(64, 32, kernel_size = 3, padding = 1, stride = 2)    # convolution | 8*8*64 -> 4*4*32 | RF = 55

        # convolution block 4
        self.conv4a = nn.Conv2d(32, 10, kernel_size = 3)                             # convolution | 4*4*32 -> 2*2*10 | RF = 71

        
        # batch normalization for conv block 1
        self.batch_norm1a = nn.BatchNorm2d(64)  
        self.batch_norm1b = nn.BatchNorm2d(64)  
        self.batch_norm1c = nn.BatchNorm2d(32)  

        # batch normalization for conv block 2
        self.batch_norm2a = nn.BatchNorm2d(64)  
        self.batch_norm2b = nn.BatchNorm2d(32)  
        self.batch_norm2c = nn.BatchNorm2d(64)  
        self.batch_norm2d = nn.BatchNorm2d(32)    

        # batch normalization for conv block 3
        self.batch_norm3a = nn.BatchNorm2d(64)  
        self.batch_norm3b = nn.BatchNorm2d(32) 
        self.batch_norm3c = nn.BatchNorm2d(64)  
        self.batch_norm3d = nn.BatchNorm2d(32)  
 
        # dropout
        self.dropout = nn.Dropout(0.05)

        # global average pooling
        self.gap = nn.AdaptiveAvgPool2d(1)  # global average pooling | 2*2*10 -> 1*1*10 | RF = 79


    def forward(self, img):
        # conv block 1
        x = F.relu(self.batch_norm1a(self.conv1a(img)))   # convolution | 32*32*3 -> 32*32*128 | RF = 3
        x = F.relu(self.batch_norm1b(self.conv1b(x)))     # convolution | 32*32*128 -> 32*32*64 | RF = 5
        x = F.relu(self.batch_norm1c(self.conv1c(x)))     # convolution | 32*32*64 -> 16*16*64 | RF = 7
        x = self.dropout(x)                               # dropout

        # conv block 2
        x = F.relu(self.batch_norm2a(self.conv2a(x)))     # convolution | 16*16*32 -> 16*16*64 | RF = 11
        x = F.relu(self.batch_norm2b(self.conv2b(x)))     # convolution | 16*16*64 -> 16*16*32 | RF = 11 
        x = self.dropout(x)                               # droput
        x = F.relu(self.batch_norm2c(self.conv2c(x)))     # convolution | 16*16*64 -> 16*16*32 | RF = 19    
        x = self.dropout(x)                               # dropout
        x = F.relu(self.batch_norm2d(self.conv2d(x)))     # convolution | 16*16*64 -> 8*8*32 | RF = 23
        x = self.dropout(x)                               # dropout

        # conv block 3
        x = F.relu(self.batch_norm3a(self.conv3a(x)))     # convolution | 8*8*32 -> 8*8*64 | RF = 31
        x = F.relu(self.batch_norm3b(self.conv3b(x)))     # convolution | 8*8*64 -> 8*8*32 | RF = 31
        x = self.dropout(x)   
        x = F.relu(self.batch_norm3c(self.conv3c(x)))     # convolution | 8*8*64 -> 8*8*32 | RF = 47 
        x = self.dropout(x)  
        x = F.relu(self.batch_norm3d(self.conv3d(x)))     # convolution | 8*8*64 -> 4*4*32 | RF = 55
        x = self.dropout(x)
        
        #conv block 4
        x = self.conv4a(x)                                # convolution | 4*4*32 -> 2*2*10 | RF = 71  

        # global average pooling
        x = self.gap(x)                                   # global average pooling | 2*2*10 -> 1*1*10 | RF = 79
        x = x.view(-1, 10)  

        # output layer
        img_out = F.log_softmax(x, dim = 1)               # output layer to output probabilities for label

        return img_out